# Analysis

## Layer 1, Head 4

In the sentence the attention head appears to be capturing the relationship between the verb "be" and the hidden term, helping to predict the grammatical context of the sentence 1.
For 2 sentence, the diagonal is lighter, indicating that words are often self-attending. In the 2 sentence the masked word “[MASK]” receives significant attention from the word “can,” suggesting that the model is trying to associate the term “[MASK]” with something that is possible to do or related to the verb “can.”

Example Sentences:
- Attention heads can be [MASK], so they won’t always have clear human interpretations.
- For human interpretations attention heads can be [MASK].

## Layer 3, Head 5

The attention head in this layer appears to be focused on capturing the relationships between verbs and head nouns in the sentence to predict masked words. It helps maintain grammatical and semantic coherence by paying attention to the key words that connect the sentence structure to the [MASK] token.

Example Sentences:
- The power of imagination makes us [MASK].
- To keep your balance, you must keep [MASK].


